#!/bin/bash
#
# Script per generare i manifest del workload cluster CAPV usando variabili d'ambiente
#
# Usage:
#   1. Carica i parametri: source ~/.cluster-api/vsphere-params.env
#   2. Esegui script: ./scripts/generate-cluster-manifest.sh
#   3. Applica: kubectl apply -f generated/cluster-full.yaml
#

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Funzione per stampare messaggi colorati
info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
    exit 1
}

# Verifica che le variabili d'ambiente siano caricate
check_vars() {
    local missing_vars=()

    # Variabili richieste
    local required_vars=(
        "VSPHERE_SERVER"
        "VSPHERE_USERNAME"
        "VSPHERE_PASSWORD"
        "VSPHERE_THUMBPRINT"
        "VSPHERE_DATACENTER"
        "VSPHERE_DATASTORE"
        "VSPHERE_NETWORK"
        "VSPHERE_RESOURCE_POOL"
        "VSPHERE_FOLDER"
        "VSPHERE_TEMPLATE"
        "VSPHERE_SSH_AUTHORIZED_KEY"
        "CLUSTER_NAME"
        "KUBERNETES_VERSION"
        "CONTROL_PLANE_ENDPOINT_IP"
    )

    for var in "${required_vars[@]}"; do
        if [ -z "${!var:-}" ]; then
            missing_vars+=("$var")
        fi
    done

    if [ ${#missing_vars[@]} -gt 0 ]; then
        error "Missing required environment variables:\n  - ${missing_vars[*]}\n\nPlease run: source ~/.cluster-api/vsphere-params.env"
    fi

    info "All required variables are set ✓"
}

# Crea directory output se non esiste
prepare_output() {
    local output_dir="generated"
    if [ ! -d "$output_dir" ]; then
        mkdir -p "$output_dir"
        info "Created output directory: $output_dir"
    fi
}

# Genera manifest
generate_manifest() {
    local output_file="generated/cluster-full.yaml"

    info "Generating cluster manifest..."
    info "  Cluster name: ${CLUSTER_NAME}"
    info "  Kubernetes version: ${KUBERNETES_VERSION}"
    info "  Control plane endpoint: ${CONTROL_PLANE_ENDPOINT_IP}:6443"
    info "  vSphere server: ${VSPHERE_SERVER}"

    cat > "$output_file" <<EOF
---
# Generated by generate-cluster-manifest.sh
# Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
# Cluster: ${CLUSTER_NAME}
# Kubernetes: ${KUBERNETES_VERSION}

---
apiVersion: v1
kind: Namespace
metadata:
  name: ${CLUSTER_NAME}
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: default
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - ${POD_CIDR:-10.244.0.0/16}
    services:
      cidrBlocks:
        - ${SERVICE_CIDR:-10.96.0.0/12}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: VSphereCluster
    name: ${CLUSTER_NAME}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VSphereCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${CONTROL_PLANE_ENDPOINT_IP}
    port: 6443
  identityRef:
    kind: VSphereClusterIdentity
    name: ${CLUSTER_NAME}-vsphere-identity
  server: ${VSPHERE_SERVER}
  thumbprint: "${VSPHERE_THUMBPRINT}"

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VSphereClusterIdentity
metadata:
  name: ${CLUSTER_NAME}-vsphere-identity
  namespace: default
spec:
  secretName: ${CLUSTER_NAME}-vsphere-creds
  allowedNamespaces:
    selector:
      matchLabels: {}

---
apiVersion: v1
kind: Secret
metadata:
  name: ${CLUSTER_NAME}-vsphere-creds
  namespace: default
type: Opaque
stringData:
  username: "${VSPHERE_USERNAME}"
  password: "${VSPHERE_PASSWORD}"

---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: default
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:-1}
  version: ${KUBERNETES_VERSION}
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: VSphereMachineTemplate
      name: ${CLUSTER_NAME}-control-plane-template
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: external
        timeoutForControlPlane: 10m
      controllerManager:
        extraArgs:
          cloud-provider: external
          allocate-node-cidrs: "false"
      imageRepository: registry.k8s.io
      networking:
        dnsDomain: cluster.local
        podSubnet: ${POD_CIDR:-10.244.0.0/16}
        serviceSubnet: ${SERVICE_CIDR:-10.96.0.0/12}
    initConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
        name: "{{ ds.meta_data.hostname }}"
    joinConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
        name: "{{ ds.meta_data.hostname }}"
    preKubeadmCommands:
      - swapoff -a
      - sed -i '/ swap / s/^/#/' /etc/fstab
      - modprobe overlay
      - modprobe br_netfilter
      - sysctl -w net.ipv4.ip_forward=1
      - sysctl -w net.bridge.bridge-nf-call-iptables=1
    users:
      - name: capv
        sshAuthorizedKeys:
          - "${VSPHERE_SSH_AUTHORIZED_KEY}"
        sudo: ALL=(ALL) NOPASSWD:ALL

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VSphereMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane-template
  namespace: default
spec:
  template:
    spec:
      cloneMode: ${VSPHERE_CLONE_MODE:-linkedClone}
      datacenter: ${VSPHERE_DATACENTER}
      datastore: ${VSPHERE_DATASTORE}
      diskGiB: ${VSPHERE_CONTROL_PLANE_DISK_GIB:-50}
      folder: ${VSPHERE_FOLDER}
      memoryMiB: ${VSPHERE_CONTROL_PLANE_MEM_MIB:-8192}
      network:
        devices:
          - dhcp4: true
            networkName: ${VSPHERE_NETWORK}
      numCPUs: ${VSPHERE_CONTROL_PLANE_NUM_CPUS:-2}
      os: linux
      powerOffMode: trySoft
      resourcePool: ${VSPHERE_RESOURCE_POOL}
      server: ${VSPHERE_SERVER}
      storagePolicyName: ""
      template: ${VSPHERE_TEMPLATE}
      thumbprint: "${VSPHERE_THUMBPRINT}"

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-workers
  namespace: default
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT:-2}
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      cluster.x-k8s.io/deployment-name: ${CLUSTER_NAME}-workers
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
        cluster.x-k8s.io/deployment-name: ${CLUSTER_NAME}-workers
        node-role.kubernetes.io/worker: ""
    spec:
      version: ${KUBERNETES_VERSION}
      clusterName: ${CLUSTER_NAME}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-worker-bootstrap
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: VSphereMachineTemplate
        name: ${CLUSTER_NAME}-worker-template

---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-worker-bootstrap
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          criSocket: /var/run/containerd/containerd.sock
          kubeletExtraArgs:
            cloud-provider: external
            node-labels: "node.kubernetes.io/worker="
          name: "{{ ds.meta_data.hostname }}"
      preKubeadmCommands:
        - swapoff -a
        - sed -i '/ swap / s/^/#/' /etc/fstab
        - modprobe overlay
        - modprobe br_netfilter
        - sysctl -w net.ipv4.ip_forward=1
        - sysctl -w net.bridge.bridge-nf-call-iptables=1
      users:
        - name: capv
          sshAuthorizedKeys:
            - "${VSPHERE_SSH_AUTHORIZED_KEY}"
          sudo: ALL=(ALL) NOPASSWD:ALL

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: VSphereMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-worker-template
  namespace: default
spec:
  template:
    spec:
      cloneMode: ${VSPHERE_CLONE_MODE:-linkedClone}
      datacenter: ${VSPHERE_DATACENTER}
      datastore: ${VSPHERE_DATASTORE}
      diskGiB: ${VSPHERE_WORKER_DISK_GIB:-100}
      folder: ${VSPHERE_FOLDER}
      memoryMiB: ${VSPHERE_WORKER_MEM_MIB:-16384}
      network:
        devices:
          - dhcp4: true
            networkName: ${VSPHERE_NETWORK}
      numCPUs: ${VSPHERE_WORKER_NUM_CPUS:-4}
      os: linux
      powerOffMode: trySoft
      resourcePool: ${VSPHERE_RESOURCE_POOL}
      server: ${VSPHERE_SERVER}
      storagePolicyName: ""
      template: ${VSPHERE_TEMPLATE}
      thumbprint: "${VSPHERE_THUMBPRINT}"
EOF

    info "Manifest generated successfully: $output_file"
}

# Valida il manifest
validate_manifest() {
    local output_file="generated/cluster-full.yaml"

    info "Validating manifest..."

    # Verifica che il file esista
    if [ ! -f "$output_file" ]; then
        error "Manifest file not found: $output_file"
    fi

    # Verifica sintassi YAML
    if command -v yamllint &> /dev/null; then
        yamllint -d relaxed "$output_file" || warn "yamllint found issues (non-critical)"
    else
        warn "yamllint not installed, skipping YAML validation"
    fi

    # Dry-run kubectl (se disponibile e connesso al cluster)
    if command -v kubectl &> /dev/null && kubectl cluster-info &> /dev/null; then
        info "Running kubectl dry-run..."
        kubectl apply -f "$output_file" --dry-run=client || warn "kubectl dry-run found issues"
    else
        warn "kubectl not available or not connected, skipping dry-run validation"
    fi

    info "Validation complete ✓"
}

# Print summary
print_summary() {
    local output_file="generated/cluster-full.yaml"

    echo ""
    echo "=========================================="
    echo "Cluster Manifest Generated Successfully!"
    echo "=========================================="
    echo ""
    echo "Output file: $output_file"
    echo ""
    echo "Cluster configuration:"
    echo "  Name: ${CLUSTER_NAME}"
    echo "  Kubernetes version: ${KUBERNETES_VERSION}"
    echo "  Control plane: ${CONTROL_PLANE_MACHINE_COUNT:-1} node(s) @ ${VSPHERE_CONTROL_PLANE_NUM_CPUS:-2} vCPU, ${VSPHERE_CONTROL_PLANE_MEM_MIB:-8192} MiB"
    echo "  Workers: ${WORKER_MACHINE_COUNT:-2} node(s) @ ${VSPHERE_WORKER_NUM_CPUS:-4} vCPU, ${VSPHERE_WORKER_MEM_MIB:-16384} MiB"
    echo "  Endpoint: ${CONTROL_PLANE_ENDPOINT_IP}:6443"
    echo ""
    echo "Next steps:"
    echo "  1. Review the manifest:"
    echo "     cat $output_file"
    echo ""
    echo "  2. Apply to management cluster:"
    echo "     export KUBECONFIG=~/.kube/srv26-config"
    echo "     kubectl apply -f $output_file"
    echo ""
    echo "  3. Monitor cluster creation:"
    echo "     watch kubectl get clusters,machines,vspheremachines"
    echo ""
    echo "  4. Get kubeconfig when ready:"
    echo "     clusterctl get kubeconfig ${CLUSTER_NAME} > ~/.kube/${CLUSTER_NAME}-config"
    echo ""
}

# Main
main() {
    info "Starting cluster manifest generation..."

    check_vars
    prepare_output
    generate_manifest
    validate_manifest
    print_summary
}

main "$@"
