# Cluster RKE2 - homelab-k8s
# Cluster Kubernetes workload su vSphere gestito da Rancher
#
# IMPORTANTE: Rancher 2.13.1 ha un bug in CAPI che genera errori "no matching controller owner ref"
# quando si crea il cluster via API/CLI o UI. La creazione via Rancher non funziona.
# Soluzione: Provisioning manuale + registrazione in Rancher.
#
# Questo file serve come documentazione della configurazione target.
#
# Struttura cartelle vSphere:
#   Kubernetes VMs/
#   ├── k8s-management/    -> VM Rancher (srv26)
#   ├── k8s-data-plane/    -> Control Plane nodes
#   └── k8s-workers/       -> Worker nodes
#
# Datastore:
#   - datastore02-local-raid (SSD) -> OS delle VM
#   - datastore03-local-raid (HDD) -> PersistentVolume (vSphere CSI)
#
# Network VLANs:
#   - VLAN 35 (net-k8s-management): Control plane, API server
#   - VLAN 36 (net-k8s-workload): Pod traffic, node communication
#   - VLAN 37 (net-k8s-services): MetalLB LoadBalancer IPs
#
# Template VM:
#   - Flatcar Container Linux: Control Plane e Workers standard
#   - Ubuntu 24.04 LTS: Solo GPU Workers (per driver NVIDIA GRID)

apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: homelab-k8s
  namespace: fleet-default
  annotations:
    field.cattle.io/description: "Homelab Kubernetes workload cluster on vSphere"
spec:
  # Cloud credential reference - usa il secret vsphere-credentials
  cloudCredentialSecretName: cattle-global-data:vsphere-credentials

  # Kubernetes version - RKE2
  kubernetesVersion: v1.34.2+rke2r1

  # Accesso diretto al cluster (bypass Rancher proxy)
  localClusterAuthEndpoint:
    enabled: true
    fqdn: k8s.mosca.lan

  rkeConfig:
    # Cilium CNI con Hubble UI per osservabilità
    chartValues:
      rke2-cilium:
        hubble:
          enabled: true
          relay:
            enabled: true
          ui:
            enabled: true

    # Backup etcd
    etcd:
      disableSnapshots: false
      snapshotRetention: 5
      snapshotScheduleCron: 0 */5 * * *

    # Configurazione globale
    machineGlobalConfig:
      cni: cilium
      disable-kube-proxy: false
      etcd-expose-metrics: false
      # Disabilitiamo nginx ingress di default, useremo il nostro
      disable:
        - rke2-ingress-nginx

    # Machine Pools
    machinePools:
      #=========================================================================
      # CONTROL PLANE - Flatcar Container Linux
      #=========================================================================
      # - 1 nodo (single control plane per homelab)
      # - unhealthyNodeTimeout: 0m = NON ricreare automaticamente
      # - Folder: k8s-data-plane
      # - Network: VLAN 35 (management)
      - name: control-plane
        etcdRole: true
        controlPlaneRole: true
        workerRole: false
        quantity: 1
        unhealthyNodeTimeout: 0m
        drainBeforeDelete: true
        machineOS: linux
        machineConfigRef:
          kind: VmwarevsphereConfig
          name: homelab-control-plane
        labels:
          node-role.kubernetes.io/control-plane: "true"

      #=========================================================================
      # WORKERS STANDARD - Photon OS 5.0
      #=========================================================================
      # - 2 nodi
      # - unhealthyNodeTimeout: 5m
      # - Folder: k8s-workers
      # - Network: VLAN 36+37 (workload + services)
      - name: workers
        etcdRole: false
        controlPlaneRole: false
        workerRole: true
        quantity: 2
        unhealthyNodeTimeout: 5m
        drainBeforeDelete: true
        machineOS: linux
        machineConfigRef:
          kind: VmwarevsphereConfig
          name: homelab-worker-photon
        labels:
          node.kubernetes.io/pool: workers

      #=========================================================================
      # GPU WORKERS - Ubuntu 24.04 LTS (per driver NVIDIA GRID)
      #=========================================================================
      # - 0 nodi inizialmente (da aggiungere dopo configurazione vGPU)
      # - unhealthyNodeTimeout: 0m = NON ricreare (GPU legata alla VM)
      # - Folder: k8s-workers
      # - Networks: VLAN 36 + VLAN 37
      # - Taint per scheduling esclusivo GPU workloads
      - name: gpu-workers
        etcdRole: false
        controlPlaneRole: false
        workerRole: true
        quantity: 0
        unhealthyNodeTimeout: 0m
        drainBeforeDelete: true
        machineOS: linux
        machineConfigRef:
          kind: VmwarevsphereConfig
          name: homelab-worker-gpu
        labels:
          node.kubernetes.io/pool: gpu-workers
          nvidia.com/gpu: "present"
        taints:
          - key: nvidia.com/gpu
            value: "present"
            effect: NoSchedule

    machineSelectorConfig:
      - config:
          protect-kernel-defaults: false

    # Strategia di upgrade
    upgradeStrategy:
      controlPlaneConcurrency: '1'
      controlPlaneDrainOptions:
        deleteEmptyDirData: true
        disableEviction: false
        enabled: true
        gracePeriod: 30
        timeout: 300
      workerConcurrency: '1'
      workerDrainOptions:
        deleteEmptyDirData: true
        disableEviction: false
        enabled: true
        gracePeriod: 30
        timeout: 300

